# AICGSecEval

[‰∏≠ÊñáÊñáÊ°£](./README_zh.md)

current version: 1.0

## 1. Overview

AICGSecEval (AI Code Generation Security Evaluation) provides a novel repository-level AI-generated code security evaluation benchmark, aiming to assess the security performance of code generated by large language models (LLMs) by simulating real-world AI programming processes.

### Design Philosophy

Unlike traditional fragment-level code generation scenarios created manually, AICGSecEval extracts repository-level code generation scenarios from real-world GitHub repositories. In these scenarios, LLMs need to not only understand the functional prompts provided but also comprehend the contextual information of the repository to generate correct and secure code. To ensure the generated code is security-sensitive, we start with CVE patches, hollow out critical vulnerable code, and extract the necessary code context, which is provided to the LLM for code generation. The generated code is then incorporated into the original repository in the form of a patch for security evaluation.


### Benchmark Datasets

AICGSecEval 1.0 includes xx high-quality seed datasets from real-world GitHub repositories and CVE vulnerabilities, as well as xx mutated datasets. We manually perform structural and semantic mutations on the repository code in the seed datasets to mitigate the data leakage risk during LLM training, ensuring that the LLM has not previously encountered this data.

AICGSecEval 1.0 primarily focuses on web development scenarios and covers four popular vulnerability types: Cross-Site Scripting (XSS), SQL Injection, Path Traversal, and Command Injection. It involves five mainstream programming languages: Java, Python, Go, JavaScript, and PHP.

### Evaluation Metrics

AICGSecEval comprehensively evaluates the code generated by LLMs across the following three dimensions and calculates the overall performance of the LLM in the code generation task using a weighted average:

* Code Security (60%): Scans the generated code using SAST tools to detect whether the original CVE vulnerabilities still exist. To ensure the effectiveness of the security evaluation, each entry in the dataset is configured with a dedicated SAST image to accurately identify the target CVE vulnerabilities.

* Code Quality (30%): The code quality is assessed by evaluating whether the generated code can be successfully integrated into the original project and pass syntax checks performed by SAST tools.

* Generation Stability (10%): For each entry in the dataset, the LLM undergoes three rounds of code generation to assess the consistency and stability of the generated output.


### üèÖ [Leaderboard](https://aicgseceval.tencent.com/rank)

## 2. Evaluating LLM via AICGSecEval

### Environment Configuration

* Hardware Requirements: Available disk space of 50GB or more, recommended memory of 8GB or more

* Python Version: 3.11 or higher
    ```
    # Install dependencies
    pip install -r requirements.txt
    ```

* Install [docker](https://docs.docker.com/engine/install/)
    ```
    # Run the following command to test Docker environment availability
    docker pull aiseceval/ai_gen_code:latest
    ```

### Run Example

```
python invoke.py \
  --model_name="Model name to test" \ 
  --batch_id="v1.0" \ 
  --base_url="https://xxx/" \
  --api_key="Your LLM API key" \
  --github_token="Your GitHub token"
```

| Parameter Name  | Required | Description                        | Example Value                           |
| ----------------| -------- | ----------------------------------  | --------------------------------------- |
| model_name      | Required | LLM model name                      | gpt-4o-2024-11-20                       |
| batch_id        | Required | Test batch ID                       | v1.0                                    |
| base_url        | Required | LLM API service URL                 | https://api.openai.com/v1/              |
| api_key         | Required | LLM API key                         | sk-xxxxxx                               |
| github_token    | Required | GitHub access token                 | ghp_xxxxxxxx                            |
| output_dir      | Optional | Output directory                    | outputs (Default)                               |
| temperature     | Optional | Randomness parameter for text generation | 0.2 (Uses the default server configuration by default)      |
| top_p           | Optional | Diversity parameter for text generation | 0.8 (Uses the default server configuration by default)     |
| max_context_token | Optional | Maximum tokens for input prompt     | 64000 (Default)               |
| max_gen_token   | Optional | Maximum tokens for generated text   | 64000 (Default)                        |
| model_args      | Optional | Model parameters (JSON format string) | {"temperature": 0.2, "top_p": 0.8}    |
| max_workers    | Optional  | Maximum Concurrency (SAST Scan)   | 1 (Default) |

Evaluation result output file: `{output_dir}/{model_name}_{batch_id}_eval_result.txt`

Note: The full evaluation is time-consuming. Users can increase the max_workers based on their hardware specifications to speed up the process. Additionally, the tool has a built-in automatic checkpoint reconnection mechanism. If the code is interrupted, users only need to rerun it to resume execution.


### LLM Call Support
This project currently supports LLM services that conform to the OpenAI API standard. For other customized LLM calling methods, you can modify the `call_llm()` function in `bench/generate_code.py` to implement custom call logic.

### Submit to Leaderboard
If you are interested in submitting your model to our leaderboard, please follow the instructions posted in [TencentAISec/experiments](https://github.com/TencentAISec/experiments/blob/main/README.md).

## 3. Future Plans

We will continue to optimize and enhance the project features. The future optimization plan includes but is not limited to the following aspects. We welcome active discussions and suggestions from the community.
* Dataset Expansion: Support more vulnerability types (e.g., OWASP Top 10), programming languages, and application scenarios.
* Evaluation Methodology Optimization:
    * Introduce more advanced code context extraction algorithms (current algorithm: BM25).
	* Implement a dynamic PoC-based security evaluation framework to improve evaluation accuracy.
* Leaderboard Optimization: Support model capability comparisons across more dimensions and granularities


We sincerely welcome suggestions and contributions from the community!
* Report Issues: [Submit an Issue](https://github.com/Tencent/AICGSecEval/issues)
* Submit Code: [Create a Pull Request](https://github.com/Tencent/AICGSecEval/pulls)



## 4. License
This project is open source under the Apache-2.0 License. For more details, please refer to the [License.txt](./License.txt) file.